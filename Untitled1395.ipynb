{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d65b2f-5175-40a0-84c6-6346610932e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PolyGlotAI Artifacts Ready ===\n",
      "PKL  -> C:\\Users\\sagni\\Downloads\\Poly Glot AI\\polyglot_model.pkl\n",
      "H5   -> C:\\Users\\sagni\\Downloads\\Poly Glot AI\\polyglot_model.h5\n",
      "YAML -> C:\\Users\\sagni\\Downloads\\Poly Glot AI\\polyglot_config.yaml\n",
      "JSON -> C:\\Users\\sagni\\Downloads\\Poly Glot AI\\polyglot_report.json\n",
      "Rows used: 5 | Mode: unsupervised\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import yaml\n",
    "import h5py\n",
    "\n",
    "# Audio feature extraction\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Paths (defaults)\n",
    "# =========================\n",
    "DEFAULT_AUDIO_DIR = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\\archive\\audio\"\n",
    "DEFAULT_TEXT_DIR  = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\\archive\\text\"\n",
    "DEFAULT_CSV_PATH  = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\\archive\\British English Speech Recognition.csv\"\n",
    "DEFAULT_OUT_DIR   = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\"\n",
    "\n",
    "POSSIBLE_LABELS = [\n",
    "    \"label\", \"Label\", \"target\", \"Target\", \"accent\", \"Accent\",\n",
    "    \"speaker_id\", \"Speaker\", \"class\", \"Class\"\n",
    "]\n",
    "POSSIBLE_TEXT_COLS = [\"transcript\", \"text\", \"utterance\", \"sentence\", \"phrase\", \"Transcript\", \"Text\"]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def list_basenames(root, exts=(\"*.wav\",\"*.WAV\")):\n",
    "    \"\"\"Return dict: basename (without ext) -> full path for first match.\"\"\"\n",
    "    mapping = {}\n",
    "    for pat in exts:\n",
    "        for path in glob.glob(os.path.join(root, pat)):\n",
    "            base = os.path.splitext(os.path.basename(path))[0]\n",
    "            mapping[base] = path\n",
    "    return mapping\n",
    "\n",
    "def list_basenames_txt(root, exts=(\"*.txt\",\"*.TXT\")):\n",
    "    mapping = {}\n",
    "    for pat in exts:\n",
    "        for path in glob.glob(os.path.join(root, pat)):\n",
    "            base = os.path.splitext(os.path.basename(path))[0]\n",
    "            mapping[base] = path\n",
    "    return mapping\n",
    "\n",
    "def safe_read_txt(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read().strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def save_h5_from_pickle(pkl_path: str, h5_path: str, meta: dict):\n",
    "    \"\"\"Create an HDF5 file embedding the pickled model bytes (+ metadata).\"\"\"\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        model_bytes = f.read()\n",
    "    with h5py.File(h5_path, \"w\") as h5:\n",
    "        dset = h5.create_dataset(\"model_pickle\", data=np.void(model_bytes))\n",
    "        for k, v in meta.items():\n",
    "            try:\n",
    "                if isinstance(v, (str, int, float, np.integer, np.floating)):\n",
    "                    dset.attrs[k] = v\n",
    "                else:\n",
    "                    dset.attrs[k] = json.dumps(v)\n",
    "            except Exception:\n",
    "                dset.attrs[k] = str(v)\n",
    "\n",
    "def robust_load_wav(path, sr=16000):\n",
    "    \"\"\"\n",
    "    Prefer soundfile (no librosa deprecation), fallback to librosa.load.\n",
    "    Returns (y, sr). On failure, (None, None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, r = sf.read(path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            if y.ndim > 1:\n",
    "                y = np.mean(y, axis=1)\n",
    "            if r != sr:\n",
    "                y = librosa.resample(y.astype(float), orig_sr=r, target_sr=sr)\n",
    "                r = sr\n",
    "        return y.astype(float), r\n",
    "    except Exception:\n",
    "        try:\n",
    "            y, r = librosa.load(path, sr=sr, mono=True)\n",
    "            return y, r\n",
    "        except Exception:\n",
    "            return None, None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pickle-safe Transformers\n",
    "# =========================\n",
    "class Squeeze1D(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Ensure a pandas column slice becomes a 1-D array (n_samples,).\"\"\"\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        return np.asarray(X, dtype=object).ravel()\n",
    "\n",
    "class TextStatExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Simple numeric text stats: char_len, word_count, avg_word_len.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names_ = np.array([\"char_len\", \"word_count\", \"avg_word_len\"])\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=object).ravel().tolist()\n",
    "        rows = []\n",
    "        for t in X:\n",
    "            t = t if isinstance(t, str) else \"\"\n",
    "            chars = len(t)\n",
    "            words = re.findall(r\"\\w+\", t, flags=re.UNICODE)\n",
    "            wcnt = len(words)\n",
    "            avgw = (sum(len(w) for w in words) / wcnt) if wcnt else 0.0\n",
    "            rows.append([chars, wcnt, avgw])\n",
    "        return np.array(rows, dtype=float)\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.feature_names_\n",
    "\n",
    "class AudioFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extract compact audio features from file paths:\n",
    "      - duration, rms mean/std, zcr mean, spectral centroid mean/std,\n",
    "        MFCC 13 means + 13 stds  (total 6 + 26 = 32 dims)\n",
    "    \"\"\"\n",
    "    def __init__(self, sr=16000, n_mfcc=13):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        names = [\"dur_sec\", \"rms_mean\", \"rms_std\", \"zcr_mean\", \"spec_cent_mean\", \"spec_cent_std\"]\n",
    "        names += [f\"mfcc{i+1}_mean\" for i in range(self.n_mfcc)]\n",
    "        names += [f\"mfcc{i+1}_std\" for i in range(self.n_mfcc)]\n",
    "        self.feature_names_ = names\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=object).ravel().tolist()\n",
    "        feats = []\n",
    "        for wav_path in X:\n",
    "            try:\n",
    "                y, sr = robust_load_wav(wav_path, sr=self.sr)\n",
    "                if y is None or sr is None or len(y) == 0:\n",
    "                    raise RuntimeError(\"Audio load failed\")\n",
    "\n",
    "                dur = len(y) / float(sr)\n",
    "\n",
    "                rms = librosa.feature.rms(y=y).flatten()\n",
    "                zcr = librosa.feature.zero_crossing_rate(y).flatten()\n",
    "                spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr).flatten()\n",
    "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc)\n",
    "\n",
    "                vec = [\n",
    "                    dur,\n",
    "                    float(np.mean(rms)) if rms.size else 0.0,\n",
    "                    float(np.std(rms))  if rms.size else 0.0,\n",
    "                    float(np.mean(zcr)) if zcr.size else 0.0,\n",
    "                    float(np.mean(spec_cent)) if spec_cent.size else 0.0,\n",
    "                    float(np.std(spec_cent))  if spec_cent.size else 0.0,\n",
    "                ]\n",
    "                if mfcc.size:\n",
    "                    vec += list(np.mean(mfcc, axis=1))\n",
    "                    vec += list(np.std(mfcc, axis=1))\n",
    "                else:\n",
    "                    vec += [0.0]*self.n_mfcc + [0.0]*self.n_mfcc\n",
    "            except Exception:\n",
    "                vec = [0.0]*(6 + self.n_mfcc*2)\n",
    "            feats.append(vec)\n",
    "        return np.array(feats)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_ or [])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"PolyGlotAI train + export (pickle-safe)\")\n",
    "    parser.add_argument(\"--audio\", default=DEFAULT_AUDIO_DIR, help=\"Audio directory with .wav\")\n",
    "    parser.add_argument(\"--text\",  default=DEFAULT_TEXT_DIR,  help=\"Text directory with .txt\")\n",
    "    parser.add_argument(\"--csv\",   default=DEFAULT_CSV_PATH,  help=\"CSV with metadata (optional)\")\n",
    "    parser.add_argument(\"--out\",   default=DEFAULT_OUT_DIR,   help=\"Output directory for artifacts\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    os.makedirs(args.out, exist_ok=True)\n",
    "\n",
    "    # -------- Collect file lists --------\n",
    "    audio_map = list_basenames(args.audio, exts=(\"*.wav\",\"*.WAV\"))\n",
    "    text_map  = list_basenames_txt(args.text, exts=(\"*.txt\",\"*.TXT\"))\n",
    "\n",
    "    df_csv = None\n",
    "    if os.path.isfile(args.csv):\n",
    "        try:\n",
    "            df_csv = pd.read_csv(args.csv)\n",
    "        except Exception:\n",
    "            df_csv = pd.read_csv(args.csv, encoding=\"latin-1\")\n",
    "\n",
    "    # normalize csv to have 'basename' and maybe 'transcript'/'label'\n",
    "    if df_csv is not None:\n",
    "        df_csv = df_csv.copy()\n",
    "        fname_col = next((c for c in [\"filename\",\"file\",\"path\",\"wav\",\"audio\",\"fname\",\"id\",\"ID\",\"Name\",\"name\"] if c in df_csv.columns), None)\n",
    "        if fname_col is not None:\n",
    "            df_csv[\"basename\"] = df_csv[fname_col].astype(str).apply(lambda p: os.path.splitext(os.path.basename(p))[0])\n",
    "        else:\n",
    "            df_csv[\"basename\"] = df_csv.index.astype(str)\n",
    "\n",
    "        text_col = next((c for c in POSSIBLE_TEXT_COLS if c in df_csv.columns), None)\n",
    "        label_col = next((c for c in POSSIBLE_LABELS if c in df_csv.columns), None)\n",
    "    else:\n",
    "        union_basenames = sorted(set(audio_map.keys()) | set(text_map.keys()))\n",
    "        df_csv = pd.DataFrame({\"basename\": union_basenames})\n",
    "        text_col = None\n",
    "        label_col = None\n",
    "\n",
    "    # Attach resolved paths and text/transcript\n",
    "    df_csv[\"audio_path\"] = df_csv[\"basename\"].map(audio_map)\n",
    "    if 'text_value' not in df_csv.columns:\n",
    "        df_csv[\"text_value\"] = \"\"\n",
    "    if text_col:\n",
    "        df_csv[\"text_value\"] = df_csv[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "    # fill text_value from text files if empty\n",
    "    def _fill(row):\n",
    "        if row[\"text_value\"]:\n",
    "            return row[\"text_value\"]\n",
    "        p = text_map.get(row[\"basename\"])\n",
    "        return safe_read_txt(p) if p else \"\"\n",
    "    df_csv[\"text_value\"] = df_csv.apply(_fill, axis=1)\n",
    "\n",
    "    # Drop rows without audio\n",
    "    df = df_csv[df_csv[\"audio_path\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    # Determine y (label) if available\n",
    "    y = None\n",
    "    label_used = None\n",
    "    if label_col and label_col in df.columns:\n",
    "        y_series = df[label_col]\n",
    "        if y_series.dtype == object:\n",
    "            cats = {v: i for i, v in enumerate(sorted(y_series.dropna().unique()))}\n",
    "            y = y_series.map(cats)\n",
    "            label_mapping = {str(k): int(v) for k, v in cats.items()}\n",
    "        else:\n",
    "            y = y_series.copy()\n",
    "            label_mapping = None\n",
    "        label_used = label_col\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # Frame for transformers\n",
    "    X_frame = pd.DataFrame({\n",
    "        \"audio_path\": df[\"audio_path\"].astype(str).values,\n",
    "        \"text_value\": df[\"text_value\"].astype(str).values\n",
    "    })\n",
    "\n",
    "    # --------------------------\n",
    "    # Build feature + model pipeline(s) (no lambdas)\n",
    "    # --------------------------\n",
    "    audio_pipe = Pipeline([\n",
    "        (\"select\", Squeeze1D()),\n",
    "        (\"afe\", AudioFeatureExtractor(sr=16000, n_mfcc=13)),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    text_stats = TextStatExtractor()\n",
    "    text_tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(2,4), max_features=800)\n",
    "\n",
    "    # Two branches on the same source column \"text_value\"\n",
    "    text_features = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"tfidf\", Pipeline([(\"sel\", Squeeze1D()), (\"tfidf\", text_tfidf)]), \"text_value\"),\n",
    "            (\"tstats\", Pipeline([(\"sel\", Squeeze1D()), (\"tstat\", text_stats)]), \"text_value\"),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Supervised path\n",
    "    supervised_results = None\n",
    "    best_pipe = None\n",
    "    mode = \"unsupervised\"\n",
    "\n",
    "    if y is not None and pd.Series(y).nunique() >= 2:\n",
    "        mode = \"supervised\"\n",
    "        full_feat = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"audio\", audio_pipe, [\"audio_path\"]),\n",
    "                (\"text\",  text_features, [\"text_value\"])\n",
    "            ],\n",
    "            remainder=\"drop\"\n",
    "        )\n",
    "\n",
    "        models = {\n",
    "            \"logreg\": LogisticRegression(max_iter=2000),\n",
    "            \"rf\": RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1)\n",
    "        }\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_frame, y, test_size=0.25, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        supervised_results = {}\n",
    "        best_name, best_score = None, -np.inf\n",
    "\n",
    "        for name, est in models.items():\n",
    "            pipe = Pipeline([(\"features\", full_feat), (\"model\", est)])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred = pipe.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            auc = None\n",
    "            try:\n",
    "                if len(np.unique(y_test)) == 2 and hasattr(pipe, \"predict_proba\"):\n",
    "                    probs = pipe.predict_proba(X_test)[:, 1]\n",
    "                    if not np.allclose(np.min(probs), np.max(probs)):\n",
    "                        auc = roc_auc_score(y_test, probs)\n",
    "            except Exception:\n",
    "                auc = None\n",
    "\n",
    "            score = auc if auc is not None else acc\n",
    "            supervised_results[name] = {\n",
    "                \"accuracy\": float(acc),\n",
    "                \"roc_auc\": (None if auc is None else float(auc)),\n",
    "                \"chosen_score\": float(score)\n",
    "            }\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_name = name\n",
    "                best_pipe = pipe\n",
    "\n",
    "        chosen_model = best_name\n",
    "\n",
    "    # Unsupervised path (if no label)\n",
    "    if best_pipe is None:\n",
    "        mode = \"unsupervised\"\n",
    "        full_feat = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"audio\", audio_pipe, [\"audio_path\"]),\n",
    "                (\"text\",  text_features, [\"text_value\"])\n",
    "            ],\n",
    "            remainder=\"drop\"\n",
    "        )\n",
    "        kmeans = MiniBatchKMeans(n_clusters=5, random_state=42, batch_size=128, n_init=\"auto\")\n",
    "        best_pipe = Pipeline([(\"features\", full_feat), (\"cluster\", kmeans)])\n",
    "        best_pipe.fit(X_frame)\n",
    "\n",
    "        clusters = best_pipe.predict(X_frame)\n",
    "        unique, counts = np.unique(clusters, return_counts=True)\n",
    "        supervised_results = {\"clusters\": {int(k): int(v) for k, v in zip(unique, counts)}}\n",
    "        chosen_model = \"MiniBatchKMeans(k=5)\"\n",
    "\n",
    "    # --------------------------\n",
    "    # Exports\n",
    "    # --------------------------\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    pkl_path  = os.path.join(args.out, \"polyglot_model.pkl\")\n",
    "    h5_path   = os.path.join(args.out, \"polyglot_model.h5\")\n",
    "    yml_path  = os.path.join(args.out, \"polyglot_config.yaml\")\n",
    "    jsn_path  = os.path.join(args.out, \"polyglot_report.json\")\n",
    "\n",
    "    joblib.dump(best_pipe, pkl_path)\n",
    "\n",
    "    cfg = {\n",
    "        \"run\": {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"mode\": mode,\n",
    "            \"audio_dir\": args.audio,\n",
    "            \"text_dir\": args.text,\n",
    "            \"csv_path\": args.csv,\n",
    "            \"out_dir\": args.out\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"audio\": {\"sr\": 16000, \"n_mfcc\": 13},\n",
    "            \"text\":  {\"tfidf\": {\"analyzer\": \"char\", \"ngram_range\": [2,4], \"max_features\": 800},\n",
    "                      \"stats\": [\"char_len\", \"word_count\", \"avg_word_len\"]}\n",
    "        },\n",
    "        \"supervised\": {\n",
    "            \"label_column\": (label_used if mode == \"supervised\" else None),\n",
    "            \"label_mapping\": (label_mapping if mode == \"supervised\" else None),\n",
    "            \"candidates\": ([\"logreg\", \"rf\"] if mode == \"supervised\" else None),\n",
    "            \"chosen_model\": chosen_model if mode == \"supervised\" else None\n",
    "        },\n",
    "        \"unsupervised\": {\"algorithm\": \"MiniBatchKMeans\", \"n_clusters\": 5}\n",
    "    }\n",
    "    with open(yml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    # Dataset summary (audio durations)\n",
    "    dur_stats = duration_stats(df[\"audio_path\"].tolist())\n",
    "\n",
    "    report = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"mode\": mode,\n",
    "        \"rows\": int(df.shape[0]),\n",
    "        \"columns\": int(df.shape[1]),\n",
    "        \"durations_sec\": dur_stats,\n",
    "        \"results\": supervised_results,\n",
    "        \"chosen_model\": chosen_model\n",
    "    }\n",
    "    with open(jsn_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    # H5 (embed pickle)\n",
    "    h5_meta = {\"artifact_type\": \"pickle_container\", \"mode\": mode, \"chosen_model\": chosen_model, \"timestamp\": timestamp}\n",
    "    save_h5_from_pickle(pkl_path, h5_path, h5_meta)\n",
    "\n",
    "    print(\"\\n=== PolyGlotAI Artifacts Ready ===\")\n",
    "    print(\"PKL  ->\", pkl_path)\n",
    "    print(\"H5   ->\", h5_path)\n",
    "    print(\"YAML ->\", yml_path)\n",
    "    print(\"JSON ->\", jsn_path)\n",
    "    print(f\"Rows used: {df.shape[0]} | Mode: {mode}\")\n",
    "\n",
    "\n",
    "def duration_stats(paths):\n",
    "    vals = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            y, sr = robust_load_wav(p, sr=16000)\n",
    "            if y is None or sr is None or len(y) == 0:\n",
    "                continue\n",
    "            vals.append(len(y)/float(sr))\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not vals:\n",
    "        return None\n",
    "    s = pd.Series(vals)\n",
    "    return {\n",
    "        \"count\": int(s.size),\n",
    "        \"mean_sec\": float(s.mean()),\n",
    "        \"median_sec\": float(s.median()),\n",
    "        \"min_sec\": float(s.min()),\n",
    "        \"max_sec\": float(s.max()),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225ba9b-2672-4105-9210-b3d7559e901d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
