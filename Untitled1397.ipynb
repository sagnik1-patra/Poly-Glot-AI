{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d8f75e-68e0-40e1-8633-439bf1b3eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] No label column found or derived.\n",
      "Tip 1: Pass --label \"<your_column>\" (exact name from CSV headers)\n",
      "Tip 2: Derive from filename: --label_from_filename '.*_accent-([a-z]+)_.*'  (adjust regex)\n",
      "CSV columns detected: ['ID', 'Audio', 'Text', 'basename', 'audio_path', 'text_value']\n"
     ]
    }
   ],
   "source": [
    "# polyglot_accuracy_and_heatmap.py  (with --label & filename-regex support)\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, re, json, glob, argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa, soundfile as sf\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEFAULT_AUDIO_DIR = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\\archive\\audio\"\n",
    "DEFAULT_TEXT_DIR  = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\\archive\\text\"\n",
    "DEFAULT_CSV_PATH  = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\\archive\\British English Speech Recognition.csv\"\n",
    "DEFAULT_OUT_DIR   = r\"C:\\Users\\sagni\\Downloads\\Poly Glot AI\"\n",
    "\n",
    "POSSIBLE_LABELS = [\n",
    "    \"label\",\"Label\",\"target\",\"Target\",\"accent\",\"Accent\",\n",
    "    \"speaker_id\",\"Speaker\",\"class\",\"Class\",\"language\",\"Language\"\n",
    "]\n",
    "POSSIBLE_TEXT_COLS = [\"transcript\",\"Transcript\",\"text\",\"Text\",\"utterance\",\"sentence\",\"phrase\"]\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def list_basenames(root, exts):\n",
    "    out = {}\n",
    "    for pat in exts:\n",
    "        for path in glob.glob(os.path.join(root, pat)):\n",
    "            out[os.path.splitext(os.path.basename(path))[0]] = path\n",
    "    return out\n",
    "\n",
    "def read_csv_any(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")\n",
    "\n",
    "def robust_load_wav(path, sr=16000):\n",
    "    try:\n",
    "        y, r = sf.read(path, always_2d=False)\n",
    "        if isinstance(y, np.ndarray):\n",
    "            if y.ndim > 1:\n",
    "                y = np.mean(y, axis=1)\n",
    "            if r != sr:\n",
    "                y = librosa.resample(y.astype(float), orig_sr=r, target_sr=sr); r = sr\n",
    "        return y.astype(float), r\n",
    "    except Exception:\n",
    "        try:\n",
    "            y, r = librosa.load(path, sr=sr, mono=True); return y, r\n",
    "        except Exception:\n",
    "            return None, None\n",
    "\n",
    "# ----------------- Pickle-safe transformers -----------------\n",
    "class Squeeze1D(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return np.asarray(X, dtype=object).ravel()\n",
    "\n",
    "class TextStatExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names_ = np.array([\"char_len\",\"word_count\",\"avg_word_len\"]); return self\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=object).ravel().tolist()\n",
    "        rows = []\n",
    "        for t in X:\n",
    "            t = t if isinstance(t, str) else \"\"\n",
    "            words = re.findall(r\"\\w+\", t, flags=re.UNICODE)\n",
    "            rows.append([len(t), len(words), (sum(len(w) for w in words)/len(words) if words else 0.0)])\n",
    "        return np.array(rows, dtype=float)\n",
    "\n",
    "class AudioFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sr=16000, n_mfcc=13): self.sr=sr; self.n_mfcc=n_mfcc\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=object).ravel().tolist(); feats=[]\n",
    "        for p in X: feats.append(self._feat_one(p))\n",
    "        return np.array(feats)\n",
    "    def _feat_one(self, path):\n",
    "        try:\n",
    "            y, sr = robust_load_wav(path, sr=self.sr)\n",
    "            if y is None or len(y)==0: raise RuntimeError\n",
    "            dur = len(y)/float(sr)\n",
    "            rms = librosa.feature.rms(y=y).flatten()\n",
    "            zcr = librosa.feature.zero_crossing_rate(y).flatten()\n",
    "            sc  = librosa.feature.spectral_centroid(y=y, sr=sr).flatten()\n",
    "            mf  = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc)\n",
    "            v = [\n",
    "                dur,\n",
    "                float(np.mean(rms)) if rms.size else 0.0,\n",
    "                float(np.std(rms))  if rms.size else 0.0,\n",
    "                float(np.mean(zcr)) if zcr.size else 0.0,\n",
    "                float(np.mean(sc))  if sc.size else 0.0,\n",
    "                float(np.std(sc))   if sc.size else 0.0,\n",
    "            ]\n",
    "            if mf.size:\n",
    "                v += list(np.mean(mf, axis=1)); v += list(np.std(mf, axis=1))\n",
    "            else:\n",
    "                v += [0.0]*self.n_mfcc + [0.0]*self.n_mfcc\n",
    "            return v\n",
    "        except Exception:\n",
    "            return [0.0]*(6 + self.n_mfcc*2)\n",
    "\n",
    "# ----------------- Label discovery -----------------\n",
    "def normalize_cols(df):\n",
    "    return {c: re.sub(r\"\\s+\", \"\", str(c)).lower() for c in df.columns}\n",
    "\n",
    "def find_label_column(df, user_label=None):\n",
    "    norm = normalize_cols(df)  # map original->normalized\n",
    "    inv  = {v:k for k,v in norm.items()}\n",
    "    if user_label:\n",
    "        key = re.sub(r\"\\s+\", \"\", user_label).lower()\n",
    "        if key in inv: return inv[key]\n",
    "    for name in POSSIBLE_LABELS:\n",
    "        key = re.sub(r\"\\s+\", \"\", name).lower()\n",
    "        if key in inv: return inv[key]\n",
    "    # Auto-guess: low-cardinality categorical/object columns (2..min(50, n/2) uniques)\n",
    "    candidates = []\n",
    "    for col in df.columns:\n",
    "        if col in (\"basename\",\"audio_path\",\"text_value\"): continue\n",
    "        uniques = df[col].dropna().nunique()\n",
    "        if 2 <= uniques <= max(2, min(50, df.shape[0]//2)):\n",
    "            candidates.append((col, uniques, str(df[col].dtype)))\n",
    "    # prefer object/string columns\n",
    "    candidates.sort(key=lambda x: (x[2]!=\"object\", x[1]))  # object first, then fewer uniques\n",
    "    return candidates[0][0] if candidates else None\n",
    "\n",
    "def label_from_filename(df, regex, group=1):\n",
    "    pat = re.compile(regex)\n",
    "    def _ex(basename):\n",
    "        m = pat.search(basename)\n",
    "        return m.group(group) if m else None\n",
    "    return df[\"basename\"].astype(str).apply(_ex)\n",
    "\n",
    "# ----------------- Main -----------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"PolyGlotAI accuracy + heatmap (with --label / filename regex)\")\n",
    "    ap.add_argument(\"--audio\", default=DEFAULT_AUDIO_DIR)\n",
    "    ap.add_argument(\"--text\",  default=DEFAULT_TEXT_DIR)\n",
    "    ap.add_argument(\"--csv\",   default=DEFAULT_CSV_PATH)\n",
    "    ap.add_argument(\"--out\",   default=DEFAULT_OUT_DIR)\n",
    "    ap.add_argument(\"--label\", default=None, help=\"Exact column name to use as label (case/space-insensitive)\")\n",
    "    ap.add_argument(\"--label_from_filename\", default=None,\n",
    "                    help=r\"Regex with one capturing group to extract label from basename, e.g. '.*_accent-([a-z]+)_.*' or '^(.*?)-\\d+$'\")\n",
    "    ap.add_argument(\"--label_group\", type=int, default=1, help=\"Capturing group index for --label_from_filename (default 1)\")\n",
    "    args, _ = ap.parse_known_args()\n",
    "    os.makedirs(args.out, exist_ok=True)\n",
    "\n",
    "    audio_map = list_basenames(args.audio, (\"*.wav\",\"*.WAV\"))\n",
    "    text_map  = list_basenames(args.text,  (\"*.txt\",\"*.TXT\"))\n",
    "\n",
    "    df_csv = read_csv_any(args.csv) if os.path.isfile(args.csv) else pd.DataFrame()\n",
    "    if not df_csv.empty:\n",
    "        fname_col = next((c for c in [\"filename\",\"file\",\"path\",\"wav\",\"audio\",\"fname\",\"id\",\"ID\",\"Name\",\"name\"] if c in df_csv.columns), None)\n",
    "        if fname_col:\n",
    "            df_csv[\"basename\"] = df_csv[fname_col].astype(str).apply(lambda p: os.path.splitext(os.path.basename(p))[0])\n",
    "        else:\n",
    "            df_csv[\"basename\"] = df_csv.index.astype(str)\n",
    "        text_col = next((c for c in POSSIBLE_TEXT_COLS if c in df_csv.columns), None)\n",
    "    else:\n",
    "        union = sorted(set(audio_map.keys()) | set(text_map.keys()))\n",
    "        df_csv = pd.DataFrame({\"basename\": union}); text_col = None\n",
    "\n",
    "    df_csv[\"audio_path\"] = df_csv[\"basename\"].map(audio_map)\n",
    "    df_csv[\"text_value\"] = df_csv[text_col].fillna(\"\").astype(str) if text_col else \"\"\n",
    "    # fill from text folder if empty\n",
    "    def fill_txt(row):\n",
    "        if isinstance(row[\"text_value\"], str) and row[\"text_value\"]:\n",
    "            return row[\"text_value\"]\n",
    "        p = text_map.get(row[\"basename\"])\n",
    "        if p and os.path.isfile(p):\n",
    "            try: return open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\").read().strip()\n",
    "            except Exception: return \"\"\n",
    "        return \"\"\n",
    "    df_csv[\"text_value\"] = df_csv.apply(fill_txt, axis=1)\n",
    "\n",
    "    # keep rows that have audio\n",
    "    df = df_csv[df_csv[\"audio_path\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    # ---- choose / build label ----\n",
    "    label_col = None\n",
    "    if args.label_from_filename:\n",
    "        df[\"__label_from_name\"] = label_from_filename(df, args.label_from_filename, args.label_group)\n",
    "        if df[\"__label_from_name\"].notna().sum() >= 2:\n",
    "            label_col = \"__label_from_name\"\n",
    "\n",
    "    if label_col is None:\n",
    "        label_col = find_label_column(df, user_label=args.label)\n",
    "\n",
    "    if not label_col:\n",
    "        print(\"[WARN] No label column found or derived.\")\n",
    "        print(\"Tip 1: Pass --label \\\"<your_column>\\\" (exact name from CSV headers)\")\n",
    "        print(r\"Tip 2: Derive from filename: --label_from_filename '.*_accent-([a-z]+)_.*'  (adjust regex)\")\n",
    "        print(\"CSV columns detected:\", list(df.columns))\n",
    "        return\n",
    "\n",
    "    y_series = df[label_col]\n",
    "    # coerce to integers if object dtype\n",
    "    if y_series.dtype == object:\n",
    "        cats = {v: i for i, v in enumerate(sorted(y_series.dropna().unique()))}\n",
    "        y = y_series.map(cats)\n",
    "    else:\n",
    "        y = y_series.copy()\n",
    "\n",
    "    # ---- features ----\n",
    "    X_frame = pd.DataFrame({\n",
    "        \"audio_path\": df[\"audio_path\"].astype(str).values,\n",
    "        \"text_value\": df[\"text_value\"].astype(str).values\n",
    "    })\n",
    "\n",
    "    audio_pipe = Pipeline([\n",
    "        (\"sel\", Squeeze1D()),\n",
    "        (\"afe\", AudioFeatureExtractor(sr=16000, n_mfcc=13)),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    text_stats = TextStatExtractor()\n",
    "    text_tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(2,4), max_features=800)\n",
    "    text_features = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"tfidf\", Pipeline([(\"sel\", Squeeze1D()), (\"tfidf\", text_tfidf)]), \"text_value\"),\n",
    "            (\"tstats\", Pipeline([(\"sel\", Squeeze1D()), (\"tstat\", text_stats)]), \"text_value\"),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    full_feat = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"audio\", audio_pipe, [\"audio_path\"]),\n",
    "            (\"text\",  text_features, [\"text_value\"])\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    # ---- split & train ----\n",
    "    if pd.Series(y).nunique() < 2:\n",
    "        print(f\"[WARN] Label column '{label_col}' has fewer than 2 classes. Need at least 2 to compute accuracy/heatmap.\")\n",
    "        return\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_frame, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=2000),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1)\n",
    "    }\n",
    "\n",
    "    metrics = {}\n",
    "    best_name, best_score, best_pipe = None, -np.inf, None\n",
    "\n",
    "    for name, est in models.items():\n",
    "        pipe = Pipeline([(\"features\", full_feat), (\"model\", est)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        auc = None\n",
    "        try:\n",
    "            if len(np.unique(y_test)) == 2 and hasattr(pipe, \"predict_proba\"):\n",
    "                probs = pipe.predict_proba(X_test)[:, 1]\n",
    "                if not np.allclose(np.min(probs), np.max(probs)):\n",
    "                    auc = roc_auc_score(y_test, probs)\n",
    "        except Exception:\n",
    "            auc = None\n",
    "\n",
    "        chosen = auc if auc is not None else acc\n",
    "        metrics[name] = {\"accuracy\": float(acc), \"roc_auc\": (None if auc is None else float(auc)), \"chosen_score\": float(chosen)}\n",
    "        if chosen > best_score:\n",
    "            best_score, best_name, best_pipe = chosen, name, pipe\n",
    "\n",
    "    # ---- accuracy bar chart ----\n",
    "    acc_path = os.path.join(args.out, \"polyglot_accuracy.png\")\n",
    "    names = list(metrics.keys()); accs = [metrics[k][\"accuracy\"] for k in names]\n",
    "    plt.figure(figsize=(7,5), dpi=140)\n",
    "    bars = plt.bar(names, accs)\n",
    "    plt.title(\"Model Accuracy (Test Set)\")\n",
    "    plt.ylabel(\"Accuracy\"); plt.ylim(0, 1.0)\n",
    "    for b, v in zip(bars, accs):\n",
    "        plt.text(b.get_x()+b.get_width()/2, v+0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    plt.tight_layout(); plt.savefig(acc_path); plt.show()\n",
    "\n",
    "    # ---- confusion matrix heatmap ----\n",
    "    y_pred_best = best_pipe.predict(X_test)\n",
    "    labels_sorted = np.unique(np.concatenate([y_test, y_pred_best]))\n",
    "    cm = confusion_matrix(y_test, y_pred_best, labels=labels_sorted)\n",
    "\n",
    "    heat_path = os.path.join(args.out, \"polyglot_confusion_heatmap.png\")\n",
    "    plt.figure(figsize=(6,5), dpi=140)\n",
    "    im = plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"Confusion Matrix (Best: {best_name})\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(len(labels_sorted))\n",
    "    plt.xticks(ticks, labels_sorted); plt.yticks(ticks, labels_sorted)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "    thresh = cm.max()/2.0 if cm.size else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], \"d\"),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout(); plt.savefig(heat_path); plt.show()\n",
    "\n",
    "    # ---- save metrics ----\n",
    "    metrics_path = os.path.join(args.out, \"polyglot_metrics.json\")\n",
    "    out_metrics = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"audio_dir\": args.audio, \"text_dir\": args.text, \"csv_path\": args.csv,\n",
    "        \"label_column_used\": label_col,\n",
    "        \"models\": metrics, \"best_model\": best_name,\n",
    "        \"labels\": [int(x) if isinstance(x, (np.integer,)) else (x.item() if isinstance(x, np.generic) else x) for x in labels_sorted]\n",
    "    }\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out_metrics, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== Saved ===\")\n",
    "    print(\"Accuracy chart ->\", acc_path)\n",
    "    print(\"Heatmap        ->\", heat_path)\n",
    "    print(\"Metrics JSON   ->\", metrics_path)\n",
    "    print(\"Label column   ->\", label_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61e6da-df63-47be-aaa9-3a15918419ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
